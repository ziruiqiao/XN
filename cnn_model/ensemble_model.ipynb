{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "                txt                                           vertices  \\\n0               ARM  [(2942, 1224), (2980, 1228), (2979, 1239), (29...   \n1             808.3  [(464, 1188), (525, 1190), (525, 1203), (464, ...   \n2     UNPREDICTABLE  [(2464, 807), (2459, 942), (2445, 941), (2450,...   \n3             TRUTH   [(679, 381), (680, 456), (662, 456), (661, 381)]   \n4             808.3  [(706, 1177), (767, 1180), (766, 1201), (705, ...   \n...             ...                                                ...   \n1884            the   [(524, 284), (508, 302), (499, 294), (515, 276)]   \n1885      Kamkwanba     [(370, 98), (370, 162), (360, 162), (360, 98)]   \n1886             AN  [(2465, 777), (2464, 800), (2450, 799), (2451,...   \n1887            MUS  [(3236, 1232), (3274, 1235), (3273, 1247), (32...   \n1888            AND  [(1002, 348), (1004, 374), (994, 375), (992, 3...   \n\n                                               boundBox  \\\n0     {'Width': 39, 'Height': 15, 'Left': 2941, 'Top...   \n1     {'Width': 61, 'Height': 15, 'Left': 464, 'Top'...   \n2     {'Width': 19, 'Height': 135, 'Left': 2445, 'To...   \n3     {'Width': 19, 'Height': 75, 'Left': 661, 'Top'...   \n4     {'Width': 62, 'Height': 24, 'Left': 705, 'Top'...   \n...                                                 ...   \n1884  {'Width': 25, 'Height': 26, 'Left': 499, 'Top'...   \n1885  {'Width': 10, 'Height': 64, 'Left': 360, 'Top'...   \n1886  {'Width': 15, 'Height': 23, 'Left': 2450, 'Top...   \n1887  {'Width': 39, 'Height': 15, 'Left': 3235, 'Top...   \n1888  {'Width': 12, 'Height': 27, 'Left': 992, 'Top'...   \n\n                                slopes       font    word_len   direction  \\\n0         (-0.105, 11.0, -0.105, 11.0)  11.045361   38.209946  horizontal   \n1     (-0.033, 1000.0, -0.033, 1000.0)  13.000000   61.032778  horizontal   \n2           (27.0, -0.071, 26.8, -0.0)  14.017834  134.592906    vertical   \n3            (-75.0, 0.0, -75.0, -0.0)  18.000000   75.006666    vertical   \n4         (-0.049, 21.0, -0.049, 21.0)  21.023796   61.073726  horizontal   \n...                                ...        ...         ...         ...   \n1884    (1.125, -0.889, 1.125, -0.889)  12.041595   24.083189    vertical   \n1885       (1000.0, 0.0, 1000.0, -0.0)  10.000000   64.000000    vertical   \n1886        (23.0, -0.071, 22.0, -0.0)  14.017834   22.522222    vertical   \n1887      (-0.079, 12.0, -0.079, 12.0)  12.041595   38.118237  horizontal   \n1888          (-13.0, 0.1, -13.0, 0.1)  10.049876   26.076810    vertical   \n\n          center_point  Label  \\\n0     (2960.5, 1231.5)      0   \n1      (494.5, 1195.5)      0   \n2     (2454.5, 874.25)      1   \n3       (670.5, 418.5)      1   \n4      (736.0, 1189.0)      0   \n...                ...    ...   \n1884    (511.5, 289.0)      1   \n1885    (365.0, 130.0)      1   \n1886  (2457.5, 788.25)      1   \n1887  (3254.5, 1239.5)      0   \n1888    (998.0, 361.5)      1   \n\n                                               img_path   idx   type  \\\n0         ./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg  1325  train   \n1         ./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg  1024  train   \n2         ./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg  1274  train   \n3     ./docs/crop_pics/google/Bookshelves_4.jpg_1_0.jpg   660  train   \n4         ./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg  1054  train   \n...                                                 ...   ...    ...   \n1884  ./docs/crop_pics/google/Bookshelves_4.jpg_3_1.jpg   967  valid   \n1885  ./docs/crop_pics/google/Bookshelves_4.jpg_3_0.jpg   856  valid   \n1886      ./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg  1275  valid   \n1887      ./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg  1344  valid   \n1888  ./docs/crop_pics/google/Bookshelves_1.jpg_1_0.jpg   359  valid   \n\n                            crop_path  \n0        cropped_images/cropped_0.jpg  \n1        cropped_images/cropped_1.jpg  \n2        cropped_images/cropped_2.jpg  \n3        cropped_images/cropped_3.jpg  \n4        cropped_images/cropped_4.jpg  \n...                               ...  \n1884  cropped_images/cropped_1884.jpg  \n1885  cropped_images/cropped_1885.jpg  \n1886  cropped_images/cropped_1886.jpg  \n1887  cropped_images/cropped_1887.jpg  \n1888  cropped_images/cropped_1888.jpg  \n\n[1889 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>txt</th>\n      <th>vertices</th>\n      <th>boundBox</th>\n      <th>slopes</th>\n      <th>font</th>\n      <th>word_len</th>\n      <th>direction</th>\n      <th>center_point</th>\n      <th>Label</th>\n      <th>img_path</th>\n      <th>idx</th>\n      <th>type</th>\n      <th>crop_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ARM</td>\n      <td>[(2942, 1224), (2980, 1228), (2979, 1239), (29...</td>\n      <td>{'Width': 39, 'Height': 15, 'Left': 2941, 'Top...</td>\n      <td>(-0.105, 11.0, -0.105, 11.0)</td>\n      <td>11.045361</td>\n      <td>38.209946</td>\n      <td>horizontal</td>\n      <td>(2960.5, 1231.5)</td>\n      <td>0</td>\n      <td>./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg</td>\n      <td>1325</td>\n      <td>train</td>\n      <td>cropped_images/cropped_0.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>808.3</td>\n      <td>[(464, 1188), (525, 1190), (525, 1203), (464, ...</td>\n      <td>{'Width': 61, 'Height': 15, 'Left': 464, 'Top'...</td>\n      <td>(-0.033, 1000.0, -0.033, 1000.0)</td>\n      <td>13.000000</td>\n      <td>61.032778</td>\n      <td>horizontal</td>\n      <td>(494.5, 1195.5)</td>\n      <td>0</td>\n      <td>./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg</td>\n      <td>1024</td>\n      <td>train</td>\n      <td>cropped_images/cropped_1.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>UNPREDICTABLE</td>\n      <td>[(2464, 807), (2459, 942), (2445, 941), (2450,...</td>\n      <td>{'Width': 19, 'Height': 135, 'Left': 2445, 'To...</td>\n      <td>(27.0, -0.071, 26.8, -0.0)</td>\n      <td>14.017834</td>\n      <td>134.592906</td>\n      <td>vertical</td>\n      <td>(2454.5, 874.25)</td>\n      <td>1</td>\n      <td>./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg</td>\n      <td>1274</td>\n      <td>train</td>\n      <td>cropped_images/cropped_2.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRUTH</td>\n      <td>[(679, 381), (680, 456), (662, 456), (661, 381)]</td>\n      <td>{'Width': 19, 'Height': 75, 'Left': 661, 'Top'...</td>\n      <td>(-75.0, 0.0, -75.0, -0.0)</td>\n      <td>18.000000</td>\n      <td>75.006666</td>\n      <td>vertical</td>\n      <td>(670.5, 418.5)</td>\n      <td>1</td>\n      <td>./docs/crop_pics/google/Bookshelves_4.jpg_1_0.jpg</td>\n      <td>660</td>\n      <td>train</td>\n      <td>cropped_images/cropped_3.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>808.3</td>\n      <td>[(706, 1177), (767, 1180), (766, 1201), (705, ...</td>\n      <td>{'Width': 62, 'Height': 24, 'Left': 705, 'Top'...</td>\n      <td>(-0.049, 21.0, -0.049, 21.0)</td>\n      <td>21.023796</td>\n      <td>61.073726</td>\n      <td>horizontal</td>\n      <td>(736.0, 1189.0)</td>\n      <td>0</td>\n      <td>./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg</td>\n      <td>1054</td>\n      <td>train</td>\n      <td>cropped_images/cropped_4.jpg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1884</th>\n      <td>the</td>\n      <td>[(524, 284), (508, 302), (499, 294), (515, 276)]</td>\n      <td>{'Width': 25, 'Height': 26, 'Left': 499, 'Top'...</td>\n      <td>(1.125, -0.889, 1.125, -0.889)</td>\n      <td>12.041595</td>\n      <td>24.083189</td>\n      <td>vertical</td>\n      <td>(511.5, 289.0)</td>\n      <td>1</td>\n      <td>./docs/crop_pics/google/Bookshelves_4.jpg_3_1.jpg</td>\n      <td>967</td>\n      <td>valid</td>\n      <td>cropped_images/cropped_1884.jpg</td>\n    </tr>\n    <tr>\n      <th>1885</th>\n      <td>Kamkwanba</td>\n      <td>[(370, 98), (370, 162), (360, 162), (360, 98)]</td>\n      <td>{'Width': 10, 'Height': 64, 'Left': 360, 'Top'...</td>\n      <td>(1000.0, 0.0, 1000.0, -0.0)</td>\n      <td>10.000000</td>\n      <td>64.000000</td>\n      <td>vertical</td>\n      <td>(365.0, 130.0)</td>\n      <td>1</td>\n      <td>./docs/crop_pics/google/Bookshelves_4.jpg_3_0.jpg</td>\n      <td>856</td>\n      <td>valid</td>\n      <td>cropped_images/cropped_1885.jpg</td>\n    </tr>\n    <tr>\n      <th>1886</th>\n      <td>AN</td>\n      <td>[(2465, 777), (2464, 800), (2450, 799), (2451,...</td>\n      <td>{'Width': 15, 'Height': 23, 'Left': 2450, 'Top...</td>\n      <td>(23.0, -0.071, 22.0, -0.0)</td>\n      <td>14.017834</td>\n      <td>22.522222</td>\n      <td>vertical</td>\n      <td>(2457.5, 788.25)</td>\n      <td>1</td>\n      <td>./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg</td>\n      <td>1275</td>\n      <td>valid</td>\n      <td>cropped_images/cropped_1886.jpg</td>\n    </tr>\n    <tr>\n      <th>1887</th>\n      <td>MUS</td>\n      <td>[(3236, 1232), (3274, 1235), (3273, 1247), (32...</td>\n      <td>{'Width': 39, 'Height': 15, 'Left': 3235, 'Top...</td>\n      <td>(-0.079, 12.0, -0.079, 12.0)</td>\n      <td>12.041595</td>\n      <td>38.118237</td>\n      <td>horizontal</td>\n      <td>(3254.5, 1239.5)</td>\n      <td>0</td>\n      <td>./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg</td>\n      <td>1344</td>\n      <td>valid</td>\n      <td>cropped_images/cropped_1887.jpg</td>\n    </tr>\n    <tr>\n      <th>1888</th>\n      <td>AND</td>\n      <td>[(1002, 348), (1004, 374), (994, 375), (992, 3...</td>\n      <td>{'Width': 12, 'Height': 27, 'Left': 992, 'Top'...</td>\n      <td>(-13.0, 0.1, -13.0, 0.1)</td>\n      <td>10.049876</td>\n      <td>26.076810</td>\n      <td>vertical</td>\n      <td>(998.0, 361.5)</td>\n      <td>1</td>\n      <td>./docs/crop_pics/google/Bookshelves_1.jpg_1_0.jpg</td>\n      <td>359</td>\n      <td>valid</td>\n      <td>cropped_images/cropped_1888.jpg</td>\n    </tr>\n  </tbody>\n</table>\n<p>1889 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chardet\n",
    "import pandas as pd\n",
    "from ActivePyTools.grab_data import eval_object_columns\n",
    "\n",
    "def grab_df_data(df_path):\n",
    "    with open(df_path, 'rb') as file:\n",
    "        encoding = chardet.detect(file.read())['encoding']\n",
    "\n",
    "    temp_df = pd.read_csv(df_path, encoding=encoding)\n",
    "    df = eval_object_columns(temp_df)\n",
    "    return df\n",
    "\n",
    "split_df = grab_df_data('../data/split_df.csv')\n",
    "split_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T23:44:29.059768100Z",
     "start_time": "2024-05-12T23:44:27.864562900Z"
    }
   },
   "id": "2581eac34ec30da3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Classical Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "230dd08db1bc95fe"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from simple_model.LR_func import CustomLRModel\n",
    "from simple_model.model_utils import *\n",
    "loc_df, X_loc_train, y_loc_train, X_loc_valid, y_loc_valid, X_loc_test, y_loc_test = load_loc_data('../')\n",
    "vec_df, X_vec_train, y_vec_train, X_vec_valid, y_vec_valid, X_vec_test, y_vec_test = load_vec_data('../')\n",
    "\n",
    "cla_mod = CustomLRModel.load_model('../simple_model/')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T23:44:29.424285700Z",
     "start_time": "2024-05-12T23:44:29.059374500Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "txt_len             0.157895\ndirection           1.000000\nangle              89.942704\nnorm_font           0.037763\nnorm_word_len       0.080507\nnorm_center_x       0.288874\nnorm_center_y       0.392555\nfont_sqrt           0.194327\nfont_square         0.001426\nfont_cbrt           0.335497\nfont_cube           0.000054\nword_len_sqrt       0.397360\nword_len_square     0.024931\nword_len_cbrt       0.540492\nword_len_cube       0.003936\nName: 1605, dtype: float64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_loc_valid.iloc[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T23:44:29.425287200Z",
     "start_time": "2024-05-12T23:44:29.419869800Z"
    }
   },
   "id": "584492bf96673041"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load CNN Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f44902e75d0de3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_df = split_df[split_df['type'] == 'train']\n",
    "valid_df = split_df[split_df['type'] == 'valid']\n",
    "test_df = split_df[split_df['type'] == 'test']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T23:44:29.441111900Z",
     "start_time": "2024-05-12T23:44:29.425287200Z"
    }
   },
   "id": "838cb7a1ae1a35aa"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model_epoch_80.pth'\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet50(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "\n",
    "# 如果你的模型在 GPU 上训练并保存，且你现在使用 CPU 加载它，需要添加 map_location 参数\n",
    "# model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T23:44:34.652057100Z",
     "start_time": "2024-05-12T23:44:29.434114900Z"
    }
   },
   "id": "d50c0e47c3f55d1e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Load Data Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b4dc7cc1cd3ffd"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.functional import F\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def get_two_model_probs(df, X_vec_df, X_loc_df):\n",
    "    \n",
    "    cnn_probs_lst = []\n",
    "    cla_probs_lst = []\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "    \n",
    "        img_path = df.iloc[i]['crop_path']  # Assuming image path is last column\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        # label = int(df.iloc[i]['Label'])\n",
    "        \n",
    "        image = transform(image).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image = image.to(device)\n",
    "            cnn_preds = model(image)\n",
    "            # cnn_torch_probs = F.softmax(cnn_preds, dim=1)\n",
    "            cnn_probs = cnn_preds.tolist()[0]\n",
    "            cnn_probs_lst.append(cnn_probs)\n",
    "        \n",
    "        vec_input = X_vec_df.iloc[i]\n",
    "        loc_input = X_loc_df.iloc[i]\n",
    "        cla_probs = cla_mod.predict_proba(loc_input.to_frame().T, vec_input.to_frame().T)[0].tolist()[1]\n",
    "        cla_probs_lst.append(cla_probs)\n",
    "        \n",
    "    return cnn_probs_lst, cla_probs_lst"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T23:44:34.763479500Z",
     "start_time": "2024-05-12T23:44:34.715132500Z"
    }
   },
   "id": "a2595203f74a90d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b58ca5b1373ac52"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cnn_probs_lst, cla_probs_lst = get_two_model_probs(train_df, X_vec_train, X_loc_train)\n",
    "\n",
    "\n",
    "array1 = np.array(cnn_probs_lst)\n",
    "array2 = np.array(cla_probs_lst)\n",
    "\n",
    "# Concatenate arrays along the second axis (horizontal concatenation)\n",
    "X = np.column_stack((array1, array2))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(X, y_loc_train);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T23:45:34.074087800Z",
     "start_time": "2024-05-12T23:45:02.067154100Z"
    }
   },
   "id": "f8704695c5cb3ae1"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "cnn_probs_lst, cla_probs_lst = get_two_model_probs(test_df, X_vec_test, X_loc_test)\n",
    "\n",
    "array1 = np.array(cnn_probs_lst)\n",
    "array2 = np.array(cla_probs_lst)\n",
    "\n",
    "X = np.column_stack((array1, array2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T23:45:40.894688100Z",
     "start_time": "2024-05-12T23:45:34.075087700Z"
    }
   },
   "id": "3b76ce667029dcda"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "315bdfbe582ecb9"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM: \n",
      "[[ 48  16]\n",
      " [ 20 201]]\n",
      "Precision: 0.93\n",
      "Recall: 0.91\n",
      "Specificity: 0.75\n",
      "Accuracy: 0.87\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.9262672811059908,\n 0.9095022624434389,\n 0.75,\n array([[ 48,  16],\n        [ 20, 201]], dtype=int64))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = meta_model.predict(X)\n",
    "apply_metric(y_loc_test, test_pred, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T23:51:56.218165200Z",
     "start_time": "2024-05-12T23:51:56.155679600Z"
    }
   },
   "id": "4657a4975aa9fa6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dfa008690f5d10c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
