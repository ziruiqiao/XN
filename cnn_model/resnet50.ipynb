{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e475df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd8cbc",
   "metadata": {},
   "source": [
    "# data preparation:do not need to run the below codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "317dc78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('Bookshelves_5_labeled_df.csv', encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33c159e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>confidence</th>\n",
       "      <th>vertices</th>\n",
       "      <th>boundBox</th>\n",
       "      <th>slopes</th>\n",
       "      <th>font</th>\n",
       "      <th>word_len</th>\n",
       "      <th>direction</th>\n",
       "      <th>center_point</th>\n",
       "      <th>crop_idx</th>\n",
       "      <th>Left</th>\n",
       "      <th>Top</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't</td>\n",
       "      <td>100</td>\n",
       "      <td>[(1039, 566), (1066, 563), (1067, 576), (1040,...</td>\n",
       "      <td>{'Width': 28, 'Height': 16, 'Left': 1039, 'Top...</td>\n",
       "      <td>(0.111, -13.0, 0.111, -13.0)</td>\n",
       "      <td>13.038405</td>\n",
       "      <td>27.166155</td>\n",
       "      <td>horizontal</td>\n",
       "      <td>(1053.0, 571.0)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>1039</td>\n",
       "      <td>563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lik</td>\n",
       "      <td>100</td>\n",
       "      <td>[(1066, 563), (1080, 561), (1081, 574), (1067,...</td>\n",
       "      <td>{'Width': 15, 'Height': 15, 'Left': 1066, 'Top...</td>\n",
       "      <td>(0.143, -13.0, 0.143, -13.0)</td>\n",
       "      <td>13.038405</td>\n",
       "      <td>14.142136</td>\n",
       "      <td>horizontal</td>\n",
       "      <td>(1073.5, 568.5)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>1066</td>\n",
       "      <td>561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AND</td>\n",
       "      <td>100</td>\n",
       "      <td>[(1112, 307), (1172, 309), (1170, 364), (1110,...</td>\n",
       "      <td>{'Width': 62, 'Height': 57, 'Left': 1110, 'Top...</td>\n",
       "      <td>(-0.033, 27.5, -0.033, 27.5)</td>\n",
       "      <td>55.036352</td>\n",
       "      <td>60.033324</td>\n",
       "      <td>horizontal</td>\n",
       "      <td>(1141.0, 335.5)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>1110</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FALL</td>\n",
       "      <td>100</td>\n",
       "      <td>[(1119, 346), (1187, 353), (1185, 373), (1117,...</td>\n",
       "      <td>{'Width': 70, 'Height': 27, 'Left': 1117, 'Top...</td>\n",
       "      <td>(-0.103, 10.0, -0.088, 10.5)</td>\n",
       "      <td>20.597387</td>\n",
       "      <td>68.311769</td>\n",
       "      <td>horizontal</td>\n",
       "      <td>(1152.0, 359.75)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>1117</td>\n",
       "      <td>346</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOVE</td>\n",
       "      <td>100</td>\n",
       "      <td>[(1119, 502), (1183, 508), (1181, 530), (1117,...</td>\n",
       "      <td>{'Width': 66, 'Height': 28, 'Left': 1117, 'Top...</td>\n",
       "      <td>(-0.094, 11.0, -0.094, 11.0)</td>\n",
       "      <td>22.090722</td>\n",
       "      <td>64.280635</td>\n",
       "      <td>horizontal</td>\n",
       "      <td>(1150.0, 516.0)</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>1117</td>\n",
       "      <td>502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>the</td>\n",
       "      <td>100</td>\n",
       "      <td>[(1039, 286), (1034, 324), (1007, 320), (1012,...</td>\n",
       "      <td>{'Width': 32, 'Height': 41, 'Left': 1007, 'Top...</td>\n",
       "      <td>(7.6, -0.148, 7.4, -0.111)</td>\n",
       "      <td>27.230422</td>\n",
       "      <td>37.831923</td>\n",
       "      <td>vertical</td>\n",
       "      <td>(1023.0, 303.25)</td>\n",
       "      <td>(2, 1)</td>\n",
       "      <td>1007</td>\n",
       "      <td>283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>from</td>\n",
       "      <td>100</td>\n",
       "      <td>[(1048, 219), (1040, 281), (1013, 277), (1021,...</td>\n",
       "      <td>{'Width': 35, 'Height': 65, 'Left': 1013, 'Top...</td>\n",
       "      <td>(7.75, -0.148, 7.625, -0.111)</td>\n",
       "      <td>27.230422</td>\n",
       "      <td>62.018176</td>\n",
       "      <td>vertical</td>\n",
       "      <td>(1030.5, 248.25)</td>\n",
       "      <td>(2, 1)</td>\n",
       "      <td>1013</td>\n",
       "      <td>216</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>Half</td>\n",
       "      <td>100</td>\n",
       "      <td>[(1056, 159), (1049, 218), (1021, 214), (1028,...</td>\n",
       "      <td>{'Width': 35, 'Height': 63, 'Left': 1021, 'Top...</td>\n",
       "      <td>(8.429, -0.143, 8.429, -0.143)</td>\n",
       "      <td>28.284271</td>\n",
       "      <td>59.413803</td>\n",
       "      <td>vertical</td>\n",
       "      <td>(1038.5, 186.5)</td>\n",
       "      <td>(2, 1)</td>\n",
       "      <td>1021</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>One</td>\n",
       "      <td>100</td>\n",
       "      <td>[(1063, 103), (1056, 155), (1029, 151), (1035,...</td>\n",
       "      <td>{'Width': 34, 'Height': 56, 'Left': 1029, 'Top...</td>\n",
       "      <td>(7.429, -0.148, 8.667, -0.143)</td>\n",
       "      <td>27.789480</td>\n",
       "      <td>52.407024</td>\n",
       "      <td>vertical</td>\n",
       "      <td>(1045.75, 127.0)</td>\n",
       "      <td>(2, 1)</td>\n",
       "      <td>1029</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>RINGS</td>\n",
       "      <td>100</td>\n",
       "      <td>[(1422, 158), (1491, 154), (1492, 174), (1423,...</td>\n",
       "      <td>{'Width': 70, 'Height': 24, 'Left': 1422, 'Top...</td>\n",
       "      <td>(0.058, -20.0, 0.058, -20.0)</td>\n",
       "      <td>20.024984</td>\n",
       "      <td>69.115845</td>\n",
       "      <td>horizontal</td>\n",
       "      <td>(1457.0, 166.0)</td>\n",
       "      <td>(2, 1)</td>\n",
       "      <td>1422</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>655 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       txt  confidence                                           vertices  \\\n",
       "0    Don't         100  [(1039, 566), (1066, 563), (1067, 576), (1040,...   \n",
       "1      Lik         100  [(1066, 563), (1080, 561), (1081, 574), (1067,...   \n",
       "2      AND         100  [(1112, 307), (1172, 309), (1170, 364), (1110,...   \n",
       "3     FALL         100  [(1119, 346), (1187, 353), (1185, 373), (1117,...   \n",
       "4     LOVE         100  [(1119, 502), (1183, 508), (1181, 530), (1117,...   \n",
       "..     ...         ...                                                ...   \n",
       "650    the         100  [(1039, 286), (1034, 324), (1007, 320), (1012,...   \n",
       "651   from         100  [(1048, 219), (1040, 281), (1013, 277), (1021,...   \n",
       "652   Half         100  [(1056, 159), (1049, 218), (1021, 214), (1028,...   \n",
       "653    One         100  [(1063, 103), (1056, 155), (1029, 151), (1035,...   \n",
       "654  RINGS         100  [(1422, 158), (1491, 154), (1492, 174), (1423,...   \n",
       "\n",
       "                                              boundBox  \\\n",
       "0    {'Width': 28, 'Height': 16, 'Left': 1039, 'Top...   \n",
       "1    {'Width': 15, 'Height': 15, 'Left': 1066, 'Top...   \n",
       "2    {'Width': 62, 'Height': 57, 'Left': 1110, 'Top...   \n",
       "3    {'Width': 70, 'Height': 27, 'Left': 1117, 'Top...   \n",
       "4    {'Width': 66, 'Height': 28, 'Left': 1117, 'Top...   \n",
       "..                                                 ...   \n",
       "650  {'Width': 32, 'Height': 41, 'Left': 1007, 'Top...   \n",
       "651  {'Width': 35, 'Height': 65, 'Left': 1013, 'Top...   \n",
       "652  {'Width': 35, 'Height': 63, 'Left': 1021, 'Top...   \n",
       "653  {'Width': 34, 'Height': 56, 'Left': 1029, 'Top...   \n",
       "654  {'Width': 70, 'Height': 24, 'Left': 1422, 'Top...   \n",
       "\n",
       "                             slopes       font   word_len   direction  \\\n",
       "0      (0.111, -13.0, 0.111, -13.0)  13.038405  27.166155  horizontal   \n",
       "1      (0.143, -13.0, 0.143, -13.0)  13.038405  14.142136  horizontal   \n",
       "2      (-0.033, 27.5, -0.033, 27.5)  55.036352  60.033324  horizontal   \n",
       "3      (-0.103, 10.0, -0.088, 10.5)  20.597387  68.311769  horizontal   \n",
       "4      (-0.094, 11.0, -0.094, 11.0)  22.090722  64.280635  horizontal   \n",
       "..                              ...        ...        ...         ...   \n",
       "650      (7.6, -0.148, 7.4, -0.111)  27.230422  37.831923    vertical   \n",
       "651   (7.75, -0.148, 7.625, -0.111)  27.230422  62.018176    vertical   \n",
       "652  (8.429, -0.143, 8.429, -0.143)  28.284271  59.413803    vertical   \n",
       "653  (7.429, -0.148, 8.667, -0.143)  27.789480  52.407024    vertical   \n",
       "654    (0.058, -20.0, 0.058, -20.0)  20.024984  69.115845  horizontal   \n",
       "\n",
       "         center_point crop_idx  Left  Top  Label  \n",
       "0     (1053.0, 571.0)   (0, 0)  1039  563      0  \n",
       "1     (1073.5, 568.5)   (0, 0)  1066  561      0  \n",
       "2     (1141.0, 335.5)   (0, 0)  1110  307      0  \n",
       "3    (1152.0, 359.75)   (0, 0)  1117  346      0  \n",
       "4     (1150.0, 516.0)   (0, 0)  1117  502      0  \n",
       "..                ...      ...   ...  ...    ...  \n",
       "650  (1023.0, 303.25)   (2, 1)  1007  283      1  \n",
       "651  (1030.5, 248.25)   (2, 1)  1013  216      1  \n",
       "652   (1038.5, 186.5)   (2, 1)  1021  155      1  \n",
       "653  (1045.75, 127.0)   (2, 1)  1029   99      1  \n",
       "654   (1457.0, 166.0)   (2, 1)  1422  154      1  \n",
       "\n",
       "[655 rows x 13 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                txt                                           vertices  \\\n0               ARM  [(2942, 1224), (2980, 1228), (2979, 1239), (29...   \n1             808.3  [(464, 1188), (525, 1190), (525, 1203), (464, ...   \n2     UNPREDICTABLE  [(2464, 807), (2459, 942), (2445, 941), (2450,...   \n3             TRUTH   [(679, 381), (680, 456), (662, 456), (661, 381)]   \n4             808.3  [(706, 1177), (767, 1180), (766, 1201), (705, ...   \n...             ...                                                ...   \n1884            the   [(524, 284), (508, 302), (499, 294), (515, 276)]   \n1885      Kamkwanba     [(370, 98), (370, 162), (360, 162), (360, 98)]   \n1886             AN  [(2465, 777), (2464, 800), (2450, 799), (2451,...   \n1887            MUS  [(3236, 1232), (3274, 1235), (3273, 1247), (32...   \n1888            AND  [(1002, 348), (1004, 374), (994, 375), (992, 3...   \n\n                                               boundBox  \\\n0     {'Width': 39, 'Height': 15, 'Left': 2941, 'Top...   \n1     {'Width': 61, 'Height': 15, 'Left': 464, 'Top'...   \n2     {'Width': 19, 'Height': 135, 'Left': 2445, 'To...   \n3     {'Width': 19, 'Height': 75, 'Left': 661, 'Top'...   \n4     {'Width': 62, 'Height': 24, 'Left': 705, 'Top'...   \n...                                                 ...   \n1884  {'Width': 25, 'Height': 26, 'Left': 499, 'Top'...   \n1885  {'Width': 10, 'Height': 64, 'Left': 360, 'Top'...   \n1886  {'Width': 15, 'Height': 23, 'Left': 2450, 'Top...   \n1887  {'Width': 39, 'Height': 15, 'Left': 3235, 'Top...   \n1888  {'Width': 12, 'Height': 27, 'Left': 992, 'Top'...   \n\n                                slopes       font    word_len   direction  \\\n0         (-0.105, 11.0, -0.105, 11.0)  11.045361   38.209946  horizontal   \n1     (-0.033, 1000.0, -0.033, 1000.0)  13.000000   61.032778  horizontal   \n2           (27.0, -0.071, 26.8, -0.0)  14.017834  134.592906    vertical   \n3            (-75.0, 0.0, -75.0, -0.0)  18.000000   75.006666    vertical   \n4         (-0.049, 21.0, -0.049, 21.0)  21.023796   61.073726  horizontal   \n...                                ...        ...         ...         ...   \n1884    (1.125, -0.889, 1.125, -0.889)  12.041595   24.083189    vertical   \n1885       (1000.0, 0.0, 1000.0, -0.0)  10.000000   64.000000    vertical   \n1886        (23.0, -0.071, 22.0, -0.0)  14.017834   22.522222    vertical   \n1887      (-0.079, 12.0, -0.079, 12.0)  12.041595   38.118237  horizontal   \n1888          (-13.0, 0.1, -13.0, 0.1)  10.049876   26.076810    vertical   \n\n          center_point  Label  \\\n0     (2960.5, 1231.5)      0   \n1      (494.5, 1195.5)      0   \n2     (2454.5, 874.25)      1   \n3       (670.5, 418.5)      1   \n4      (736.0, 1189.0)      0   \n...                ...    ...   \n1884    (511.5, 289.0)      1   \n1885    (365.0, 130.0)      1   \n1886  (2457.5, 788.25)      1   \n1887  (3254.5, 1239.5)      0   \n1888    (998.0, 361.5)      1   \n\n                                               img_path   idx   type  \\\n0         ./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg  1325  train   \n1         ./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg  1024  train   \n2         ./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg  1274  train   \n3     ./docs/crop_pics/google/Bookshelves_4.jpg_1_0.jpg   660  train   \n4         ./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg  1054  train   \n...                                                 ...   ...    ...   \n1884  ./docs/crop_pics/google/Bookshelves_4.jpg_3_1.jpg   967  valid   \n1885  ./docs/crop_pics/google/Bookshelves_4.jpg_3_0.jpg   856  valid   \n1886      ./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg  1275  valid   \n1887      ./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg  1344  valid   \n1888  ./docs/crop_pics/google/Bookshelves_1.jpg_1_0.jpg   359  valid   \n\n                            crop_path  \n0        cropped_images/cropped_0.jpg  \n1        cropped_images/cropped_1.jpg  \n2        cropped_images/cropped_2.jpg  \n3        cropped_images/cropped_3.jpg  \n4        cropped_images/cropped_4.jpg  \n...                               ...  \n1884  cropped_images/cropped_1884.jpg  \n1885  cropped_images/cropped_1885.jpg  \n1886  cropped_images/cropped_1886.jpg  \n1887  cropped_images/cropped_1887.jpg  \n1888  cropped_images/cropped_1888.jpg  \n\n[1889 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>txt</th>\n      <th>vertices</th>\n      <th>boundBox</th>\n      <th>slopes</th>\n      <th>font</th>\n      <th>word_len</th>\n      <th>direction</th>\n      <th>center_point</th>\n      <th>Label</th>\n      <th>img_path</th>\n      <th>idx</th>\n      <th>type</th>\n      <th>crop_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ARM</td>\n      <td>[(2942, 1224), (2980, 1228), (2979, 1239), (29...</td>\n      <td>{'Width': 39, 'Height': 15, 'Left': 2941, 'Top...</td>\n      <td>(-0.105, 11.0, -0.105, 11.0)</td>\n      <td>11.045361</td>\n      <td>38.209946</td>\n      <td>horizontal</td>\n      <td>(2960.5, 1231.5)</td>\n      <td>0</td>\n      <td>./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg</td>\n      <td>1325</td>\n      <td>train</td>\n      <td>cropped_images/cropped_0.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>808.3</td>\n      <td>[(464, 1188), (525, 1190), (525, 1203), (464, ...</td>\n      <td>{'Width': 61, 'Height': 15, 'Left': 464, 'Top'...</td>\n      <td>(-0.033, 1000.0, -0.033, 1000.0)</td>\n      <td>13.000000</td>\n      <td>61.032778</td>\n      <td>horizontal</td>\n      <td>(494.5, 1195.5)</td>\n      <td>0</td>\n      <td>./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg</td>\n      <td>1024</td>\n      <td>train</td>\n      <td>cropped_images/cropped_1.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>UNPREDICTABLE</td>\n      <td>[(2464, 807), (2459, 942), (2445, 941), (2450,...</td>\n      <td>{'Width': 19, 'Height': 135, 'Left': 2445, 'To...</td>\n      <td>(27.0, -0.071, 26.8, -0.0)</td>\n      <td>14.017834</td>\n      <td>134.592906</td>\n      <td>vertical</td>\n      <td>(2454.5, 874.25)</td>\n      <td>1</td>\n      <td>./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg</td>\n      <td>1274</td>\n      <td>train</td>\n      <td>cropped_images/cropped_2.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRUTH</td>\n      <td>[(679, 381), (680, 456), (662, 456), (661, 381)]</td>\n      <td>{'Width': 19, 'Height': 75, 'Left': 661, 'Top'...</td>\n      <td>(-75.0, 0.0, -75.0, -0.0)</td>\n      <td>18.000000</td>\n      <td>75.006666</td>\n      <td>vertical</td>\n      <td>(670.5, 418.5)</td>\n      <td>1</td>\n      <td>./docs/crop_pics/google/Bookshelves_4.jpg_1_0.jpg</td>\n      <td>660</td>\n      <td>train</td>\n      <td>cropped_images/cropped_3.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>808.3</td>\n      <td>[(706, 1177), (767, 1180), (766, 1201), (705, ...</td>\n      <td>{'Width': 62, 'Height': 24, 'Left': 705, 'Top'...</td>\n      <td>(-0.049, 21.0, -0.049, 21.0)</td>\n      <td>21.023796</td>\n      <td>61.073726</td>\n      <td>horizontal</td>\n      <td>(736.0, 1189.0)</td>\n      <td>0</td>\n      <td>./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg</td>\n      <td>1054</td>\n      <td>train</td>\n      <td>cropped_images/cropped_4.jpg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1884</th>\n      <td>the</td>\n      <td>[(524, 284), (508, 302), (499, 294), (515, 276)]</td>\n      <td>{'Width': 25, 'Height': 26, 'Left': 499, 'Top'...</td>\n      <td>(1.125, -0.889, 1.125, -0.889)</td>\n      <td>12.041595</td>\n      <td>24.083189</td>\n      <td>vertical</td>\n      <td>(511.5, 289.0)</td>\n      <td>1</td>\n      <td>./docs/crop_pics/google/Bookshelves_4.jpg_3_1.jpg</td>\n      <td>967</td>\n      <td>valid</td>\n      <td>cropped_images/cropped_1884.jpg</td>\n    </tr>\n    <tr>\n      <th>1885</th>\n      <td>Kamkwanba</td>\n      <td>[(370, 98), (370, 162), (360, 162), (360, 98)]</td>\n      <td>{'Width': 10, 'Height': 64, 'Left': 360, 'Top'...</td>\n      <td>(1000.0, 0.0, 1000.0, -0.0)</td>\n      <td>10.000000</td>\n      <td>64.000000</td>\n      <td>vertical</td>\n      <td>(365.0, 130.0)</td>\n      <td>1</td>\n      <td>./docs/crop_pics/google/Bookshelves_4.jpg_3_0.jpg</td>\n      <td>856</td>\n      <td>valid</td>\n      <td>cropped_images/cropped_1885.jpg</td>\n    </tr>\n    <tr>\n      <th>1886</th>\n      <td>AN</td>\n      <td>[(2465, 777), (2464, 800), (2450, 799), (2451,...</td>\n      <td>{'Width': 15, 'Height': 23, 'Left': 2450, 'Top...</td>\n      <td>(23.0, -0.071, 22.0, -0.0)</td>\n      <td>14.017834</td>\n      <td>22.522222</td>\n      <td>vertical</td>\n      <td>(2457.5, 788.25)</td>\n      <td>1</td>\n      <td>./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg</td>\n      <td>1275</td>\n      <td>valid</td>\n      <td>cropped_images/cropped_1886.jpg</td>\n    </tr>\n    <tr>\n      <th>1887</th>\n      <td>MUS</td>\n      <td>[(3236, 1232), (3274, 1235), (3273, 1247), (32...</td>\n      <td>{'Width': 39, 'Height': 15, 'Left': 3235, 'Top...</td>\n      <td>(-0.079, 12.0, -0.079, 12.0)</td>\n      <td>12.041595</td>\n      <td>38.118237</td>\n      <td>horizontal</td>\n      <td>(3254.5, 1239.5)</td>\n      <td>0</td>\n      <td>./docs/crop_pics/google/IMG_7940.jpeg_0_0.jpg</td>\n      <td>1344</td>\n      <td>valid</td>\n      <td>cropped_images/cropped_1887.jpg</td>\n    </tr>\n    <tr>\n      <th>1888</th>\n      <td>AND</td>\n      <td>[(1002, 348), (1004, 374), (994, 375), (992, 3...</td>\n      <td>{'Width': 12, 'Height': 27, 'Left': 992, 'Top'...</td>\n      <td>(-13.0, 0.1, -13.0, 0.1)</td>\n      <td>10.049876</td>\n      <td>26.076810</td>\n      <td>vertical</td>\n      <td>(998.0, 361.5)</td>\n      <td>1</td>\n      <td>./docs/crop_pics/google/Bookshelves_1.jpg_1_0.jpg</td>\n      <td>359</td>\n      <td>valid</td>\n      <td>cropped_images/cropped_1888.jpg</td>\n    </tr>\n  </tbody>\n</table>\n<p>1889 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chardet\n",
    "import pandas as pd\n",
    "from ActivePyTools.grab_data import eval_object_columns\n",
    "\n",
    "def grab_df_data(df_path):\n",
    "    with open(df_path, 'rb') as file:\n",
    "        encoding = chardet.detect(file.read())['encoding']\n",
    "\n",
    "    temp_df = pd.read_csv(df_path, encoding=encoding)\n",
    "    df = eval_object_columns(temp_df)\n",
    "    return df\n",
    "\n",
    "split_df = grab_df_data('../data/split_df.csv')\n",
    "split_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T21:42:33.471649900Z",
     "start_time": "2024-05-12T21:42:32.744268800Z"
    }
   },
   "id": "ce60d0e8626d38e9"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38b9c33b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T21:16:53.565542500Z",
     "start_time": "2024-05-12T21:16:53.552532400Z"
    }
   },
   "outputs": [],
   "source": [
    "def crop_image_from_vertices(image_path, vertices, expand=10):\n",
    "    \"\"\"\n",
    "    Crop image based on vertices with expansion.\n",
    "    Arguments:\n",
    "    - image_path: str, path to the image file\n",
    "    - vertices: list of tuples, each tuple is a coordinate (x, y)\n",
    "    - expand: int, number of pixels to expand the crop area\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    min_x = min([v[0] for v in vertices])\n",
    "    min_y = min([v[1] for v in vertices])\n",
    "    max_x = max([v[0] for v in vertices])\n",
    "    max_y = max([v[1] for v in vertices])\n",
    "    \n",
    "    # Expand the box by 'expand' pixels\n",
    "    min_x = max(0, min_x - expand)\n",
    "    min_y = max(0, min_y - expand)\n",
    "    max_x = min(img.width, max_x + expand)\n",
    "    max_y = min(img.height, max_y + expand)\n",
    "    \n",
    "    cropped_image = img.crop((min_x, min_y, max_x, max_y))\n",
    "    return cropped_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "split_df_copy = split_df.copy()\n",
    "\n",
    "for index, row in split_df_copy.iterrows():\n",
    "    path = '../' + row.img_path\n",
    "    vertices = row.vertices\n",
    "    cropped_image = crop_image_from_vertices(path, vertices, expand=20)  # You can adjust the expansion here\n",
    "    crop_path = f'cropped_images/cropped_{index}.jpg'\n",
    "    cropped_image.save(crop_path)  # Save each cropped image\n",
    "    split_df.loc[index, 'crop_path'] = crop_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T21:23:21.482371300Z",
     "start_time": "2024-05-12T21:22:20.081790100Z"
    }
   },
   "id": "d986c8ac5dd3892"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "split_df.to_csv('../data/split_df.csv', index=False, encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T21:41:45.408952900Z",
     "start_time": "2024-05-12T21:41:45.362685500Z"
    }
   },
   "id": "34594720d1fde2f5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de886b45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T21:42:44.451059300Z",
     "start_time": "2024-05-12T21:42:42.353037100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826c74c",
   "metadata": {},
   "source": [
    "# Start running codes from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab602475",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T21:42:50.349149300Z",
     "start_time": "2024-05-12T21:42:48.111023300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_df = split_df[split_df['type'] == 'train']\n",
    "valid_df = split_df[split_df['type'] == 'valid']\n",
    "test_df = split_df[split_df['type'] == 'test']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T21:44:13.301992500Z",
     "start_time": "2024-05-12T21:44:13.289787100Z"
    }
   },
   "id": "780cf15962266b6e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GitHub\\XN\\cnn_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T21:43:48.522388900Z",
     "start_time": "2024-05-12T21:43:48.489602100Z"
    }
   },
   "id": "977dd77dc0f01dc7"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f78f18ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T21:53:31.597624100Z",
     "start_time": "2024-05-12T21:53:31.588918600Z"
    }
   },
   "outputs": [],
   "source": [
    "class BookshelfDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['crop_path']  # Assuming image path is last column\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = int(self.dataframe.iloc[idx]['Label'])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Transforms for the input data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = BookshelfDataset(train_df, transform=transform)\n",
    "valid_dataset = BookshelfDataset(valid_df, transform=transform)\n",
    "test_dataset = BookshelfDataset(test_df, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1127d8e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T21:53:31.871539600Z",
     "start_time": "2024-05-12T21:53:31.869542600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (1320, 13)\n",
      "Validation data size: (284, 13)\n",
      "Test data size: (285, 13)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data size: {train_df.shape}\")\n",
    "print(f\"Validation data size: {valid_df.shape}\")\n",
    "print(f\"Test data size: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f42a3c44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T22:01:14.917511200Z",
     "start_time": "2024-05-12T22:01:14.899191900Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "def apply_metric(y_actual, preds, enable_print=True):\n",
    "\n",
    "    cm = confusion_matrix(y_actual, preds)\n",
    "    TN = cm[0, 0]\n",
    "    FP = cm[0, 1]\n",
    "\n",
    "    precision = precision_score(y_actual, preds, zero_division=0)\n",
    "    recall = recall_score(y_actual, preds)\n",
    "    specificity = TN / (TN + FP)\n",
    "    accuracy = accuracy_score(y_actual, preds)\n",
    "\n",
    "    if enable_print:\n",
    "        print(f\"CM: \\n{cm}\")\n",
    "        print(f'Precision: {precision:.2f}')\n",
    "        print(f'Recall: {recall:.2f}')\n",
    "        print(f'Specificity: {specificity:.2f}')\n",
    "        print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "    return precision, recall, specificity, cm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T22:01:17.113459300Z",
     "start_time": "2024-05-12T22:01:17.102286800Z"
    }
   },
   "id": "b2309c4cb7bd9c57"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cf30e5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T22:01:17.368575300Z",
     "start_time": "2024-05-12T22:01:17.356058600Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicts = []\n",
    "    label_lst = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predicts.extend(predicted.cpu().numpy())\n",
    "            label_lst.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, predicts, label_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "894c2341",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T22:08:09.805429400Z",
     "start_time": "2024-05-12T22:08:09.789188900Z"
    }
   },
   "outputs": [],
   "source": [
    "learn_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=learn_rate)  # 只优化新的全连接层\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, num_epochs=50, lr=0.0001):\n",
    "    best_accuracy = 0\n",
    "    last_loss = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            accuracy, predicts, label_lst = evaluate_model(model, valid_loader)\n",
    "            apply_metric(label_lst, predicts)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Accuracy: {accuracy}%')\n",
    "            if last_loss != 0:\n",
    "                if loss.item() > last_loss:\n",
    "                    lr /= 2\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = lr\n",
    "            last_loss = loss.item()\n",
    "            \n",
    "            # Save the model if the accuracy is the best\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n",
    "                print(f\"Best model saved with accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4c39f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T22:01:13.363452Z",
     "start_time": "2024-05-12T21:57:19.003294800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM: \n",
      "[[ 38  26]\n",
      " [ 35 185]]\n",
      "Precision: 0.88\n",
      "Recall: 0.84\n",
      "Specificity: 0.59\n",
      "Accuracy: 0.79\n",
      "Epoch [10/100], Loss: 0.2559574246406555, Accuracy: 78.52112676056338%\n",
      "Best model saved with accuracy: 78.52112676056338%\n",
      "CM: \n",
      "[[ 37  27]\n",
      " [ 33 187]]\n",
      "Precision: 0.87\n",
      "Recall: 0.85\n",
      "Specificity: 0.58\n",
      "Accuracy: 0.79\n",
      "Epoch [20/100], Loss: 0.27055805921554565, Accuracy: 78.87323943661971%\n",
      "Best model saved with accuracy: 78.87323943661971%\n",
      "CM: \n",
      "[[ 39  25]\n",
      " [ 33 187]]\n",
      "Precision: 0.88\n",
      "Recall: 0.85\n",
      "Specificity: 0.61\n",
      "Accuracy: 0.80\n",
      "Epoch [30/100], Loss: 0.27194634079933167, Accuracy: 79.5774647887324%\n",
      "Best model saved with accuracy: 79.5774647887324%\n",
      "CM: \n",
      "[[ 37  27]\n",
      " [ 27 193]]\n",
      "Precision: 0.88\n",
      "Recall: 0.88\n",
      "Specificity: 0.58\n",
      "Accuracy: 0.81\n",
      "Epoch [40/100], Loss: 0.23584172129631042, Accuracy: 80.98591549295774%\n",
      "Best model saved with accuracy: 80.98591549295774%\n",
      "CM: \n",
      "[[ 37  27]\n",
      " [ 29 191]]\n",
      "Precision: 0.88\n",
      "Recall: 0.87\n",
      "Specificity: 0.58\n",
      "Accuracy: 0.80\n",
      "Epoch [50/100], Loss: 0.28420206904411316, Accuracy: 80.28169014084507%\n",
      "CM: \n",
      "[[ 40  24]\n",
      " [ 35 185]]\n",
      "Precision: 0.89\n",
      "Recall: 0.84\n",
      "Specificity: 0.62\n",
      "Accuracy: 0.79\n",
      "Epoch [60/100], Loss: 0.3334513306617737, Accuracy: 79.22535211267606%\n",
      "CM: \n",
      "[[ 40  24]\n",
      " [ 34 186]]\n",
      "Precision: 0.89\n",
      "Recall: 0.85\n",
      "Specificity: 0.62\n",
      "Accuracy: 0.80\n",
      "Epoch [70/100], Loss: 0.2854158878326416, Accuracy: 79.5774647887324%\n",
      "CM: \n",
      "[[ 37  27]\n",
      " [ 29 191]]\n",
      "Precision: 0.88\n",
      "Recall: 0.87\n",
      "Specificity: 0.58\n",
      "Accuracy: 0.80\n",
      "Epoch [80/100], Loss: 0.2973152995109558, Accuracy: 80.28169014084507%\n",
      "CM: \n",
      "[[ 41  23]\n",
      " [ 34 186]]\n",
      "Precision: 0.89\n",
      "Recall: 0.85\n",
      "Specificity: 0.64\n",
      "Accuracy: 0.80\n",
      "Epoch [90/100], Loss: 0.19740906357765198, Accuracy: 79.92957746478874%\n",
      "CM: \n",
      "[[ 41  23]\n",
      " [ 34 186]]\n",
      "Precision: 0.89\n",
      "Recall: 0.85\n",
      "Specificity: 0.64\n",
      "Accuracy: 0.80\n",
      "Epoch [100/100], Loss: 0.16420447826385498, Accuracy: 79.92957746478874%\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, valid_loader, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM: \n",
      "[[ 41  23]\n",
      " [ 35 185]]\n",
      "Precision: 0.89\n",
      "Recall: 0.84\n",
      "Specificity: 0.64\n",
      "Accuracy: 0.80\n",
      "Epoch [10/200], Loss: 0.1498676836490631, Accuracy: 79.5774647887324%\n",
      "Best model saved with accuracy: 79.5774647887324%\n",
      "CM: \n",
      "[[ 39  25]\n",
      " [ 31 189]]\n",
      "Precision: 0.88\n",
      "Recall: 0.86\n",
      "Specificity: 0.61\n",
      "Accuracy: 0.80\n",
      "Epoch [20/200], Loss: 0.2574372887611389, Accuracy: 80.28169014084507%\n",
      "Best model saved with accuracy: 80.28169014084507%\n",
      "CM: \n",
      "[[ 41  23]\n",
      " [ 36 184]]\n",
      "Precision: 0.89\n",
      "Recall: 0.84\n",
      "Specificity: 0.64\n",
      "Accuracy: 0.79\n",
      "Epoch [30/200], Loss: 0.1372232437133789, Accuracy: 79.22535211267606%\n",
      "CM: \n",
      "[[ 40  24]\n",
      " [ 37 183]]\n",
      "Precision: 0.88\n",
      "Recall: 0.83\n",
      "Specificity: 0.62\n",
      "Accuracy: 0.79\n",
      "Epoch [40/200], Loss: 0.21462222933769226, Accuracy: 78.52112676056338%\n",
      "CM: \n",
      "[[ 41  23]\n",
      " [ 33 187]]\n",
      "Precision: 0.89\n",
      "Recall: 0.85\n",
      "Specificity: 0.64\n",
      "Accuracy: 0.80\n",
      "Epoch [50/200], Loss: 0.20011842250823975, Accuracy: 80.28169014084507%\n",
      "CM: \n",
      "[[ 40  24]\n",
      " [ 29 191]]\n",
      "Precision: 0.89\n",
      "Recall: 0.87\n",
      "Specificity: 0.62\n",
      "Accuracy: 0.81\n",
      "Epoch [60/200], Loss: 0.24134209752082825, Accuracy: 81.33802816901408%\n",
      "Best model saved with accuracy: 81.33802816901408%\n",
      "CM: \n",
      "[[ 41  23]\n",
      " [ 34 186]]\n",
      "Precision: 0.89\n",
      "Recall: 0.85\n",
      "Specificity: 0.64\n",
      "Accuracy: 0.80\n",
      "Epoch [70/200], Loss: 0.26057183742523193, Accuracy: 79.92957746478874%\n",
      "CM: \n",
      "[[ 41  23]\n",
      " [ 35 185]]\n",
      "Precision: 0.89\n",
      "Recall: 0.84\n",
      "Specificity: 0.64\n",
      "Accuracy: 0.80\n",
      "Epoch [80/200], Loss: 0.2642403841018677, Accuracy: 79.5774647887324%\n",
      "CM: \n",
      "[[ 40  24]\n",
      " [ 31 189]]\n",
      "Precision: 0.89\n",
      "Recall: 0.86\n",
      "Specificity: 0.62\n",
      "Accuracy: 0.81\n",
      "Epoch [90/200], Loss: 0.37643635272979736, Accuracy: 80.63380281690141%\n",
      "CM: \n",
      "[[ 40  24]\n",
      " [ 32 188]]\n",
      "Precision: 0.89\n",
      "Recall: 0.85\n",
      "Specificity: 0.62\n",
      "Accuracy: 0.80\n",
      "Epoch [100/200], Loss: 0.372073233127594, Accuracy: 80.28169014084507%\n",
      "CM: \n",
      "[[ 39  25]\n",
      " [ 31 189]]\n",
      "Precision: 0.88\n",
      "Recall: 0.86\n",
      "Specificity: 0.61\n",
      "Accuracy: 0.80\n",
      "Epoch [110/200], Loss: 0.32003435492515564, Accuracy: 80.28169014084507%\n",
      "CM: \n",
      "[[ 41  23]\n",
      " [ 34 186]]\n",
      "Precision: 0.89\n",
      "Recall: 0.85\n",
      "Specificity: 0.64\n",
      "Accuracy: 0.80\n",
      "Epoch [120/200], Loss: 0.1816820651292801, Accuracy: 79.92957746478874%\n",
      "CM: \n",
      "[[ 40  24]\n",
      " [ 30 190]]\n",
      "Precision: 0.89\n",
      "Recall: 0.86\n",
      "Specificity: 0.62\n",
      "Accuracy: 0.81\n",
      "Epoch [130/200], Loss: 0.296091765165329, Accuracy: 80.98591549295774%\n",
      "CM: \n",
      "[[ 40  24]\n",
      " [ 35 185]]\n",
      "Precision: 0.89\n",
      "Recall: 0.84\n",
      "Specificity: 0.62\n",
      "Accuracy: 0.79\n",
      "Epoch [140/200], Loss: 0.16599729657173157, Accuracy: 79.22535211267606%\n",
      "CM: \n",
      "[[ 40  24]\n",
      " [ 33 187]]\n",
      "Precision: 0.89\n",
      "Recall: 0.85\n",
      "Specificity: 0.62\n",
      "Accuracy: 0.80\n",
      "Epoch [150/200], Loss: 0.20800788700580597, Accuracy: 79.92957746478874%\n",
      "CM: \n",
      "[[ 40  24]\n",
      " [ 34 186]]\n",
      "Precision: 0.89\n",
      "Recall: 0.85\n",
      "Specificity: 0.62\n",
      "Accuracy: 0.80\n",
      "Epoch [160/200], Loss: 0.14492858946323395, Accuracy: 79.5774647887324%\n",
      "CM: \n",
      "[[ 39  25]\n",
      " [ 29 191]]\n",
      "Precision: 0.88\n",
      "Recall: 0.87\n",
      "Specificity: 0.61\n",
      "Accuracy: 0.81\n",
      "Epoch [170/200], Loss: 0.22849206626415253, Accuracy: 80.98591549295774%\n",
      "CM: \n",
      "[[ 41  23]\n",
      " [ 31 189]]\n",
      "Precision: 0.89\n",
      "Recall: 0.86\n",
      "Specificity: 0.64\n",
      "Accuracy: 0.81\n",
      "Epoch [180/200], Loss: 0.16397391259670258, Accuracy: 80.98591549295774%\n",
      "CM: \n",
      "[[ 40  24]\n",
      " [ 31 189]]\n",
      "Precision: 0.89\n",
      "Recall: 0.86\n",
      "Specificity: 0.62\n",
      "Accuracy: 0.81\n",
      "Epoch [190/200], Loss: 0.16829822957515717, Accuracy: 80.63380281690141%\n",
      "CM: \n",
      "[[ 40  24]\n",
      " [ 34 186]]\n",
      "Precision: 0.89\n",
      "Recall: 0.85\n",
      "Specificity: 0.62\n",
      "Accuracy: 0.80\n",
      "Epoch [200/200], Loss: 0.20896515250205994, Accuracy: 79.5774647887324%\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, valid_loader, num_epochs=200)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T22:16:13.507694900Z",
     "start_time": "2024-05-12T22:08:15.585034100Z"
    }
   },
   "id": "7b4e2213cbd50471"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fc07450bbea43f0a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
