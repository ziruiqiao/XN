{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:10.514073100Z",
     "start_time": "2024-04-02T17:09:10.494287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from transformers import LayoutLMv3ForTokenClassification, LayoutLMv3TokenizerFast\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cpu\")  # Use the first GPU\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "          txt  confidence                                           vertices  \\\n0        ated         100         [(0, 187), (74, 194), (71, 226), (0, 219)]   \n1       808.3         100  [(184, 1168), (242, 1169), (242, 1183), (184, ...   \n2         BUC         100  [(185, 1193), (218, 1194), (218, 1207), (185, ...   \n3           A         100   [(219, 349), (219, 384), (187, 384), (187, 349)]   \n4    Writer's         100   [(219, 398), (221, 557), (189, 557), (187, 398)]   \n..        ...         ...                                                ...   \n421    POETRY         100  [(3871, 560), (3859, 638), (3839, 635), (3851,...   \n422        OF         100  [(3878, 518), (3874, 543), (3853, 540), (3857,...   \n423      BOOK         100  [(3888, 451), (3880, 503), (3860, 500), (3868,...   \n424       THE         100  [(3896, 396), (3890, 436), (3870, 432), (3876,...   \n425       Sor         100  [(3963, 228), (4030, 229), (4030, 298), (3963,...   \n\n                                              boundBox           slopes  \\\n0    {'Width': 74, 'Height': 39, 'Left': 0, 'Top': ...  (0.034, -3.832)   \n1    {'Width': 58, 'Height': 15, 'Left': 184, 'Top'...  (0.006, -300.0)   \n2    {'Width': 33, 'Height': 14, 'Left': 185, 'Top'...  (0.011, -300.0)   \n3    {'Width': 32, 'Height': 35, 'Left': 187, 'Top'...   (-300.0, -0.0)   \n4    {'Width': 34, 'Height': 159, 'Left': 187, 'Top...   (28.558, -0.0)   \n..                                                 ...              ...   \n421  {'Width': 32, 'Height': 81, 'Left': 3839, 'Top...  (-2.335, 0.054)   \n422  {'Width': 25, 'Height': 28, 'Left': 3853, 'Top...  (-2.245, 0.051)   \n423  {'Width': 28, 'Height': 55, 'Left': 3860, 'Top...  (-2.335, 0.054)   \n424  {'Width': 26, 'Height': 43, 'Left': 3870, 'Top...  (-2.395, 0.072)   \n425  {'Width': 67, 'Height': 70, 'Left': 3963, 'Top...  (0.005, -300.0)   \n\n         width      height   direction         mid_point crop_idx  Left   Top  \n0    32.140317   74.330344  horizontal    (36.25, 206.5)   (0, 0)     0   187  \n1    14.000000   58.008620  horizontal   (213.0, 1175.5)   (0, 0)   184  1168  \n2    13.000000   33.015148  horizontal   (201.5, 1200.0)   (0, 0)   185  1193  \n3    32.000000   35.000000    vertical    (203.0, 366.5)   (0, 0)   187   349  \n4    32.000000  159.012578    vertical    (204.0, 477.5)   (0, 0)   187   398  \n..         ...         ...         ...               ...      ...   ...   ...  \n421  20.223748   78.917679    vertical   (3855.0, 597.5)   (0, 0)  3839   557  \n422  21.213203   25.317978    vertical   (3865.5, 529.0)   (0, 0)  3853   515  \n423  20.223748   52.611786    vertical   (3874.0, 475.5)   (0, 0)  3860   448  \n424  20.396078   40.447497    vertical  (3883.0, 414.25)   (0, 0)  3870   393  \n425  69.000000   67.007462  horizontal   (3996.5, 263.0)   (0, 0)  3963   228  \n\n[426 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>txt</th>\n      <th>confidence</th>\n      <th>vertices</th>\n      <th>boundBox</th>\n      <th>slopes</th>\n      <th>width</th>\n      <th>height</th>\n      <th>direction</th>\n      <th>mid_point</th>\n      <th>crop_idx</th>\n      <th>Left</th>\n      <th>Top</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ated</td>\n      <td>100</td>\n      <td>[(0, 187), (74, 194), (71, 226), (0, 219)]</td>\n      <td>{'Width': 74, 'Height': 39, 'Left': 0, 'Top': ...</td>\n      <td>(0.034, -3.832)</td>\n      <td>32.140317</td>\n      <td>74.330344</td>\n      <td>horizontal</td>\n      <td>(36.25, 206.5)</td>\n      <td>(0, 0)</td>\n      <td>0</td>\n      <td>187</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>808.3</td>\n      <td>100</td>\n      <td>[(184, 1168), (242, 1169), (242, 1183), (184, ...</td>\n      <td>{'Width': 58, 'Height': 15, 'Left': 184, 'Top'...</td>\n      <td>(0.006, -300.0)</td>\n      <td>14.000000</td>\n      <td>58.008620</td>\n      <td>horizontal</td>\n      <td>(213.0, 1175.5)</td>\n      <td>(0, 0)</td>\n      <td>184</td>\n      <td>1168</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BUC</td>\n      <td>100</td>\n      <td>[(185, 1193), (218, 1194), (218, 1207), (185, ...</td>\n      <td>{'Width': 33, 'Height': 14, 'Left': 185, 'Top'...</td>\n      <td>(0.011, -300.0)</td>\n      <td>13.000000</td>\n      <td>33.015148</td>\n      <td>horizontal</td>\n      <td>(201.5, 1200.0)</td>\n      <td>(0, 0)</td>\n      <td>185</td>\n      <td>1193</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A</td>\n      <td>100</td>\n      <td>[(219, 349), (219, 384), (187, 384), (187, 349)]</td>\n      <td>{'Width': 32, 'Height': 35, 'Left': 187, 'Top'...</td>\n      <td>(-300.0, -0.0)</td>\n      <td>32.000000</td>\n      <td>35.000000</td>\n      <td>vertical</td>\n      <td>(203.0, 366.5)</td>\n      <td>(0, 0)</td>\n      <td>187</td>\n      <td>349</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Writer's</td>\n      <td>100</td>\n      <td>[(219, 398), (221, 557), (189, 557), (187, 398)]</td>\n      <td>{'Width': 34, 'Height': 159, 'Left': 187, 'Top...</td>\n      <td>(28.558, -0.0)</td>\n      <td>32.000000</td>\n      <td>159.012578</td>\n      <td>vertical</td>\n      <td>(204.0, 477.5)</td>\n      <td>(0, 0)</td>\n      <td>187</td>\n      <td>398</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>421</th>\n      <td>POETRY</td>\n      <td>100</td>\n      <td>[(3871, 560), (3859, 638), (3839, 635), (3851,...</td>\n      <td>{'Width': 32, 'Height': 81, 'Left': 3839, 'Top...</td>\n      <td>(-2.335, 0.054)</td>\n      <td>20.223748</td>\n      <td>78.917679</td>\n      <td>vertical</td>\n      <td>(3855.0, 597.5)</td>\n      <td>(0, 0)</td>\n      <td>3839</td>\n      <td>557</td>\n    </tr>\n    <tr>\n      <th>422</th>\n      <td>OF</td>\n      <td>100</td>\n      <td>[(3878, 518), (3874, 543), (3853, 540), (3857,...</td>\n      <td>{'Width': 25, 'Height': 28, 'Left': 3853, 'Top...</td>\n      <td>(-2.245, 0.051)</td>\n      <td>21.213203</td>\n      <td>25.317978</td>\n      <td>vertical</td>\n      <td>(3865.5, 529.0)</td>\n      <td>(0, 0)</td>\n      <td>3853</td>\n      <td>515</td>\n    </tr>\n    <tr>\n      <th>423</th>\n      <td>BOOK</td>\n      <td>100</td>\n      <td>[(3888, 451), (3880, 503), (3860, 500), (3868,...</td>\n      <td>{'Width': 28, 'Height': 55, 'Left': 3860, 'Top...</td>\n      <td>(-2.335, 0.054)</td>\n      <td>20.223748</td>\n      <td>52.611786</td>\n      <td>vertical</td>\n      <td>(3874.0, 475.5)</td>\n      <td>(0, 0)</td>\n      <td>3860</td>\n      <td>448</td>\n    </tr>\n    <tr>\n      <th>424</th>\n      <td>THE</td>\n      <td>100</td>\n      <td>[(3896, 396), (3890, 436), (3870, 432), (3876,...</td>\n      <td>{'Width': 26, 'Height': 43, 'Left': 3870, 'Top...</td>\n      <td>(-2.395, 0.072)</td>\n      <td>20.396078</td>\n      <td>40.447497</td>\n      <td>vertical</td>\n      <td>(3883.0, 414.25)</td>\n      <td>(0, 0)</td>\n      <td>3870</td>\n      <td>393</td>\n    </tr>\n    <tr>\n      <th>425</th>\n      <td>Sor</td>\n      <td>100</td>\n      <td>[(3963, 228), (4030, 229), (4030, 298), (3963,...</td>\n      <td>{'Width': 67, 'Height': 70, 'Left': 3963, 'Top...</td>\n      <td>(0.005, -300.0)</td>\n      <td>69.000000</td>\n      <td>67.007462</td>\n      <td>horizontal</td>\n      <td>(3996.5, 263.0)</td>\n      <td>(0, 0)</td>\n      <td>3963</td>\n      <td>228</td>\n    </tr>\n  </tbody>\n</table>\n<p>426 rows Ã— 12 columns</p>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('pic6_df.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:13.134003100Z",
     "start_time": "2024-04-02T17:09:13.109800300Z"
    }
   },
   "id": "7205254b231ef5fe"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained LayoutLMv3 model and tokenizer\n",
    "model_name = \"microsoft/layoutlmv3-base\"\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = LayoutLMv3TokenizerFast.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "image_url = './pics/IMG_7940.jpeg'\n",
    "image = Image.open(image_url)\n",
    "words = df['txt'].tolist()\n",
    "vertices = df['vertices'].tolist()\n",
    "vertices = [eval(lst) for lst in vertices]\n",
    "boxes = df['boundBox'].tolist()\n",
    "boxes = [eval(lst) for lst in boxes]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T17:09:14.474200200Z",
     "start_time": "2024-04-02T17:09:13.563495200Z"
    }
   },
   "id": "c860b85dab259db"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0, 129,  18, 156],\n        [ 45, 806,  60, 816],\n        [ 45, 823,  54, 833],\n        ...,\n        [957, 309, 964, 347],\n        [960, 271, 966, 301],\n        [983, 157, 999, 205]])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_boxes = []\n",
    "for item in boxes:\n",
    "    left = item['Left']\n",
    "    top = item['Top']\n",
    "    right = left + item['Width']\n",
    "    bottom = top + item['Height']\n",
    "    bounding_boxes.append([left, top, right, bottom])\n",
    "    \n",
    "    \n",
    "def normalize_bbox(bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Normalize bounding box coordinates to a scale of 0-1000.\n",
    "    Parameters:\n",
    "        bbox (list): The bounding box in [left, top, right, bottom] format.\n",
    "        img_width (int): The width of the image.\n",
    "        img_height (int): The height of the image.\n",
    "    Returns:\n",
    "        list: Normalized bounding box.\n",
    "    \"\"\"\n",
    "    left, top, right, bottom = bbox\n",
    "    normalized_left = (left / img_width) * 1000\n",
    "    normalized_top = (top / img_height) * 1000\n",
    "    normalized_right = (right / img_width) * 1000\n",
    "    normalized_bottom = (bottom / img_height) * 1000\n",
    "    return [normalized_left, normalized_top, normalized_right, normalized_bottom]\n",
    "\n",
    "# Assuming maximum values for width and height from the given data\n",
    "max_width = image.width\n",
    "max_height = image.height\n",
    "\n",
    "# Normalize each bounding box\n",
    "normalized_bboxes = [normalize_bbox(bbox, max_width, max_height) for bbox in bounding_boxes]\n",
    "normalized_bboxes_long = torch.tensor(normalized_bboxes, dtype=torch.long)\n",
    "normalized_bboxes_long"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T17:13:36.419636800Z",
     "start_time": "2024-04-02T17:13:36.408819900Z"
    }
   },
   "id": "a11cde632d2fc94d"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# Preprocess image and text\n",
    "encoding = tokenizer(words, boxes=normalized_bboxes_long, return_tensors=\"pt\", truncation=True, padding=\"max_length\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T17:13:36.918028100Z",
     "start_time": "2024-04-02T17:13:36.875383Z"
    }
   },
   "id": "d0373b19850df9ea"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['input_ids', 'attention_mask', 'bbox'])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T17:13:39.346416400Z",
     "start_time": "2024-04-02T17:13:39.331391500Z"
    }
   },
   "id": "e7583def463e40d8"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:993: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Move to the same device as the model\n",
    "input_ids = encoding[\"input_ids\"].to(device)\n",
    "attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "# token_type_ids = encoding[\"token_type_ids\"].to(device)\n",
    "bbox = encoding[\"bbox\"].to(model.device)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, bbox=bbox)\n",
    "    logits = outputs.logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T17:13:41.934256500Z",
     "start_time": "2024-04-02T17:13:40.713534200Z"
    }
   },
   "id": "d14b46bfd4ed50ef"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.2103, -0.9563],\n         [ 0.3301, -0.9911],\n         [ 0.0620, -0.9061],\n         ...,\n         [-0.1748, -0.1627],\n         [-0.4078,  0.2019],\n         [-0.2109, -0.9542]]])"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T17:13:50.934898600Z",
     "start_time": "2024-04-02T17:13:50.898420300Z"
    }
   },
   "id": "e113eb2845515f71"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_groups(model, tokenizer, words, boxes, threshold=0.9):\n",
    "    # Initial full prediction\n",
    "    encoding = tokenizer(words, boxes=boxes, return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
    "    inputs = {k: v.to(model.device) for k, v in encoding.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "    grouped_words = []\n",
    "    remaining_words = words[:]\n",
    "    remaining_boxes = boxes[:]\n",
    "    all_groups = []\n",
    "\n",
    "    while remaining_words:\n",
    "        predicted_classes = torch.argmax(probabilities, dim=-1)[0]\n",
    "        class_confidences = probabilities.max(dim=-1).values[0]\n",
    "\n",
    "        # Create groups for this iteration\n",
    "        current_groups = {}\n",
    "        for idx, (class_id, confidence) in enumerate(zip(predicted_classes, class_confidences)):\n",
    "            if idx in encoding.token_to_word(0) and confidence > threshold:\n",
    "                word_idx = encoding.token_to_word(0)[idx]\n",
    "                if word_idx < len(remaining_words):  # Check to avoid index errors\n",
    "                    word = remaining_words[word_idx]\n",
    "                    group_id = class_id.item()\n",
    "\n",
    "                    if group_id not in current_groups:\n",
    "                        current_groups[group_id] = []\n",
    "                    current_groups[group_id].append(word)\n",
    "                    grouped_words.append(word_idx)\n",
    "\n",
    "        # Add current groups to all groups\n",
    "        all_groups.append(current_groups)\n",
    "\n",
    "        # Prepare for next iteration by removing grouped words\n",
    "        remaining_words = [word for i, word in enumerate(remaining_words) if i not in grouped_words]\n",
    "        remaining_boxes = [box for i, box in enumerate(remaining_boxes) if i not in grouped_words]\n",
    "        if not remaining_words:\n",
    "            break  # Exit if no remaining words\n",
    "\n",
    "        # Repeat prediction with remaining words\n",
    "        encoding = tokenizer(remaining_words, boxes=remaining_boxes, return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
    "        inputs = {k: v.to(model.device) for k, v in encoding.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "    return all_groups"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T17:20:43.697752800Z",
     "start_time": "2024-04-02T17:20:43.685227Z"
    }
   },
   "id": "aafead1c468176d5"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`bbox` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:748\u001B[0m, in \u001B[0;36mBatchEncoding.convert_to_tensors\u001B[1;34m(self, tensor_type, prepend_batch_axis)\u001B[0m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tensor(value):\n\u001B[1;32m--> 748\u001B[0m     tensor \u001B[38;5;241m=\u001B[39m as_tensor(value)\n\u001B[0;32m    750\u001B[0m     \u001B[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001B[39;00m\n\u001B[0;32m    751\u001B[0m     \u001B[38;5;66;03m# # at-least2d\u001B[39;00m\n\u001B[0;32m    752\u001B[0m     \u001B[38;5;66;03m# if tensor.ndim > 2:\u001B[39;00m\n\u001B[0;32m    753\u001B[0m     \u001B[38;5;66;03m#     tensor = tensor.squeeze(0)\u001B[39;00m\n\u001B[0;32m    754\u001B[0m     \u001B[38;5;66;03m# elif tensor.ndim < 2:\u001B[39;00m\n\u001B[0;32m    755\u001B[0m     \u001B[38;5;66;03m#     tensor = tensor[None, :]\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:720\u001B[0m, in \u001B[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001B[1;34m(value, dtype)\u001B[0m\n\u001B[0;32m    719\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mtensor(np\u001B[38;5;241m.\u001B[39marray(value))\n\u001B[1;32m--> 720\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mtensor(value)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Could not infer dtype of dict",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[58], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Usage example\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m all_groups \u001B[38;5;241m=\u001B[39m predict_groups(model, tokenizer, words, boxes)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m iteration, groups \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(all_groups):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIteration \u001B[39m\u001B[38;5;132;01m{\u001B[39;00miteration\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[57], line 5\u001B[0m, in \u001B[0;36mpredict_groups\u001B[1;34m(model, tokenizer, words, boxes, threshold)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict_groups\u001B[39m(model, tokenizer, words, boxes, threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;66;03m# Initial full prediction\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m     encoding \u001B[38;5;241m=\u001B[39m tokenizer(words, boxes\u001B[38;5;241m=\u001B[39mboxes, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {k: v\u001B[38;5;241m.\u001B[39mto(model\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m encoding\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\layoutlmv3\\tokenization_layoutlmv3_fast.py:352\u001B[0m, in \u001B[0;36mLayoutLMv3TokenizerFast.__call__\u001B[1;34m(self, text, text_pair, boxes, word_labels, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_encode_plus(\n\u001B[0;32m    331\u001B[0m         batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[0;32m    332\u001B[0m         is_pair\u001B[38;5;241m=\u001B[39mis_pair,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    349\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    350\u001B[0m     )\n\u001B[0;32m    351\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 352\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[0;32m    353\u001B[0m         text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[0;32m    354\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[0;32m    355\u001B[0m         boxes\u001B[38;5;241m=\u001B[39mboxes,\n\u001B[0;32m    356\u001B[0m         word_labels\u001B[38;5;241m=\u001B[39mword_labels,\n\u001B[0;32m    357\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m    358\u001B[0m         padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[0;32m    359\u001B[0m         truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m    360\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[0;32m    361\u001B[0m         stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[0;32m    362\u001B[0m         pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[0;32m    363\u001B[0m         return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[0;32m    364\u001B[0m         return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[0;32m    365\u001B[0m         return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[0;32m    366\u001B[0m         return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[0;32m    367\u001B[0m         return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[0;32m    368\u001B[0m         return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[0;32m    369\u001B[0m         return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[0;32m    370\u001B[0m         verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m    371\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    372\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\layoutlmv3\\tokenization_layoutlmv3_fast.py:489\u001B[0m, in \u001B[0;36mLayoutLMv3TokenizerFast.encode_plus\u001B[1;34m(self, text, text_pair, boxes, word_labels, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    479\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[0;32m    480\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[0;32m    481\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[0;32m    482\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    487\u001B[0m )\n\u001B[1;32m--> 489\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encode_plus(\n\u001B[0;32m    490\u001B[0m     text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[0;32m    491\u001B[0m     boxes\u001B[38;5;241m=\u001B[39mboxes,\n\u001B[0;32m    492\u001B[0m     text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[0;32m    493\u001B[0m     word_labels\u001B[38;5;241m=\u001B[39mword_labels,\n\u001B[0;32m    494\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m    495\u001B[0m     padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[0;32m    496\u001B[0m     truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[0;32m    497\u001B[0m     max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[0;32m    498\u001B[0m     stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[0;32m    499\u001B[0m     pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[0;32m    500\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[0;32m    501\u001B[0m     return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[0;32m    502\u001B[0m     return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[0;32m    503\u001B[0m     return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[0;32m    504\u001B[0m     return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[0;32m    505\u001B[0m     return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[0;32m    506\u001B[0m     return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[0;32m    507\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m    508\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    509\u001B[0m )\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\layoutlmv3\\tokenization_layoutlmv3_fast.py:702\u001B[0m, in \u001B[0;36mLayoutLMv3TokenizerFast._encode_plus\u001B[1;34m(self, text, text_pair, boxes, word_labels, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    700\u001B[0m batched_boxes \u001B[38;5;241m=\u001B[39m [boxes]\n\u001B[0;32m    701\u001B[0m batched_word_labels \u001B[38;5;241m=\u001B[39m [word_labels] \u001B[38;5;28;01mif\u001B[39;00m word_labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 702\u001B[0m batched_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_encode_plus(\n\u001B[0;32m    703\u001B[0m     batched_input,\n\u001B[0;32m    704\u001B[0m     is_pair\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mbool\u001B[39m(text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    705\u001B[0m     boxes\u001B[38;5;241m=\u001B[39mbatched_boxes,\n\u001B[0;32m    706\u001B[0m     word_labels\u001B[38;5;241m=\u001B[39mbatched_word_labels,\n\u001B[0;32m    707\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m    708\u001B[0m     padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[0;32m    709\u001B[0m     truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[0;32m    710\u001B[0m     max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[0;32m    711\u001B[0m     stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[0;32m    712\u001B[0m     pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[0;32m    713\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[0;32m    714\u001B[0m     return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[0;32m    715\u001B[0m     return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[0;32m    716\u001B[0m     return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[0;32m    717\u001B[0m     return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[0;32m    718\u001B[0m     return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[0;32m    719\u001B[0m     return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[0;32m    720\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m    721\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    722\u001B[0m )\n\u001B[0;32m    724\u001B[0m \u001B[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001B[39;00m\n\u001B[0;32m    725\u001B[0m \u001B[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001B[39;00m\n\u001B[0;32m    726\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_tensors \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_overflowing_tokens:\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\transformers\\models\\layoutlmv3\\tokenization_layoutlmv3_fast.py:670\u001B[0m, in \u001B[0;36mLayoutLMv3TokenizerFast._batch_encode_plus\u001B[1;34m(self, batch_text_or_text_pairs, is_pair, boxes, word_labels, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001B[0m\n\u001B[0;32m    667\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_offsets_mapping:\n\u001B[0;32m    668\u001B[0m         \u001B[38;5;28;01mdel\u001B[39;00m sanitized_tokens[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moffset_mapping\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 670\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001B[38;5;241m=\u001B[39mreturn_tensors)\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:223\u001B[0m, in \u001B[0;36mBatchEncoding.__init__\u001B[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001B[0m\n\u001B[0;32m    219\u001B[0m     n_sequences \u001B[38;5;241m=\u001B[39m encoding[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mn_sequences\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_sequences \u001B[38;5;241m=\u001B[39m n_sequences\n\u001B[1;32m--> 223\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_to_tensors(tensor_type\u001B[38;5;241m=\u001B[39mtensor_type, prepend_batch_axis\u001B[38;5;241m=\u001B[39mprepend_batch_axis)\n",
      "File \u001B[1;32mD:\\Anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:764\u001B[0m, in \u001B[0;36mBatchEncoding.convert_to_tensors\u001B[1;34m(self, tensor_type, prepend_batch_axis)\u001B[0m\n\u001B[0;32m    759\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverflowing_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    760\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    761\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    762\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    763\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m--> 764\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    765\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    766\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpadding=True\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtruncation=True\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to have batched tensors with the same length. Perhaps your\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    767\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m features (`\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    768\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m expected).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    769\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    771\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[1;31mValueError\u001B[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`bbox` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "all_groups = predict_groups(model, tokenizer, words, boxes)\n",
    "for iteration, groups in enumerate(all_groups):\n",
    "    print(f\"Iteration {iteration + 1}:\")\n",
    "    for group_id, words in groups.items():\n",
    "        print(f\"  Group {group_id}: {' '.join(words)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T17:20:52.510805300Z",
     "start_time": "2024-04-02T17:20:52.325189500Z"
    }
   },
   "id": "770f89e76faded70"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "choice = [(0, 0), (1, 0), (2, 0), (0, -1)]\n",
    "probs = [0.25, 0.25, 0.25, 0.25]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T01:20:04.127744200Z",
     "start_time": "2024-04-03T01:20:04.093110600Z"
    }
   },
   "id": "ffa52b0466ed4c9b"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0, -1)]"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_choices_with_probabilities(choices=choice, prob=probs, n=1):\n",
    "    \"\"\"\n",
    "    Generate n random choices from a list of choices based on given probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    choices (list): A list of possible choices.\n",
    "    probabilities (list): A list of probabilities corresponding to each choice.\n",
    "    n (int): Number of random choices to generate.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of n random choices based on the given probabilities.\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(choices), size=n, p=prob)\n",
    "    return [choices[index] for index in indices]\n",
    "\n",
    "            \n",
    "random_choices_with_probabilities()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T01:20:05.422569700Z",
     "start_time": "2024-04-03T01:20:05.419050800Z"
    }
   },
   "id": "745740bfff277857"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d90a28ddd96ac4be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
